{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acoustic_data</th>\n",
       "      <th>time_to_failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1.4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1.4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1.4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.4691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1.4691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acoustic_data  time_to_failure\n",
       "0             12           1.4691\n",
       "1              6           1.4691\n",
       "2              8           1.4691\n",
       "3              5           1.4691\n",
       "4              8           1.4691"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"LANL-Earthquake-Prediction//train.csv\",\n",
    "                    dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4194/4194 [16:51<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a training file with simple derived features\n",
    "rows = 150_000\n",
    "segments = int(np.floor(train.shape[0] / rows))\n",
    "\n",
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "X_tr = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "\n",
    "y_tr = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n",
    "\n",
    "total_mean = train['acoustic_data'].mean()\n",
    "total_std = train['acoustic_data'].std()\n",
    "total_max = train['acoustic_data'].max()\n",
    "total_min = train['acoustic_data'].min()\n",
    "total_sum = train['acoustic_data'].sum()\n",
    "total_abs_sum = np.abs(train['acoustic_data']).sum()\n",
    "\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1]).values\n",
    "    change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return np.mean(change)\n",
    "\n",
    "for segment in tqdm(range(segments)):\n",
    "    seg = train.iloc[segment*rows:segment*rows+rows]\n",
    "    x = pd.Series(seg['acoustic_data'].values)\n",
    "    y = seg['time_to_failure'].values[-1]\n",
    "    \n",
    "    y_tr.loc[segment, 'time_to_failure'] = y\n",
    "    X_tr.loc[segment, 'mean'] = x.mean()\n",
    "    X_tr.loc[segment, 'std'] = x.std()\n",
    "    X_tr.loc[segment, 'max'] = x.max()\n",
    "    X_tr.loc[segment, 'min'] = x.min()\n",
    "    \n",
    "    X_tr.loc[segment, 'mean_change_abs'] = np.mean(np.diff(x))\n",
    "    X_tr.loc[segment, 'mean_change_rate'] = calc_change_rate(x)\n",
    "    X_tr.loc[segment, 'abs_max'] = np.abs(x).max()\n",
    "    X_tr.loc[segment, 'abs_min'] = np.abs(x).min()\n",
    "    \n",
    "    X_tr.loc[segment, 'std_first_50000'] = x[:50000].std()\n",
    "    X_tr.loc[segment, 'std_last_50000'] = x[-50000:].std()\n",
    "    X_tr.loc[segment, 'std_first_10000'] = x[:10000].std()\n",
    "    X_tr.loc[segment, 'std_last_10000'] = x[-10000:].std()\n",
    "    \n",
    "    X_tr.loc[segment, 'avg_first_50000'] = x[:50000].mean()\n",
    "    X_tr.loc[segment, 'avg_last_50000'] = x[-50000:].mean()\n",
    "    X_tr.loc[segment, 'avg_first_10000'] = x[:10000].mean()\n",
    "    X_tr.loc[segment, 'avg_last_10000'] = x[-10000:].mean()\n",
    "    \n",
    "    X_tr.loc[segment, 'min_first_50000'] = x[:50000].min()\n",
    "    X_tr.loc[segment, 'min_last_50000'] = x[-50000:].min()\n",
    "    X_tr.loc[segment, 'min_first_10000'] = x[:10000].min()\n",
    "    X_tr.loc[segment, 'min_last_10000'] = x[-10000:].min()\n",
    "    \n",
    "    X_tr.loc[segment, 'max_first_50000'] = x[:50000].max()\n",
    "    X_tr.loc[segment, 'max_last_50000'] = x[-50000:].max()\n",
    "    X_tr.loc[segment, 'max_first_10000'] = x[:10000].max()\n",
    "    X_tr.loc[segment, 'max_last_10000'] = x[-10000:].max()\n",
    "    \n",
    "    X_tr.loc[segment, 'max_to_min'] = x.max() / np.abs(x.min())\n",
    "    X_tr.loc[segment, 'max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "    X_tr.loc[segment, 'count_big'] = len(x[np.abs(x) > 500])\n",
    "    X_tr.loc[segment, 'sum'] = x.sum()\n",
    "    \n",
    "    X_tr.loc[segment, 'mean_change_rate_first_50000'] = calc_change_rate(x[:50000])\n",
    "    X_tr.loc[segment, 'mean_change_rate_last_50000'] = calc_change_rate(x[-50000:])\n",
    "    X_tr.loc[segment, 'mean_change_rate_first_10000'] = calc_change_rate(x[:10000])\n",
    "    X_tr.loc[segment, 'mean_change_rate_last_10000'] = calc_change_rate(x[-10000:])\n",
    "    \n",
    "    X_tr.loc[segment, 'q95'] = np.quantile(x, 0.95)\n",
    "    X_tr.loc[segment, 'q99'] = np.quantile(x, 0.99)\n",
    "    X_tr.loc[segment, 'q05'] = np.quantile(x, 0.05)\n",
    "    X_tr.loc[segment, 'q01'] = np.quantile(x, 0.01)\n",
    "    \n",
    "    X_tr.loc[segment, 'abs_q95'] = np.quantile(np.abs(x), 0.95)\n",
    "    X_tr.loc[segment, 'abs_q99'] = np.quantile(np.abs(x), 0.99)\n",
    "    X_tr.loc[segment, 'abs_q05'] = np.quantile(np.abs(x), 0.05)\n",
    "    X_tr.loc[segment, 'abs_q01'] = np.quantile(np.abs(x), 0.01)\n",
    "    \n",
    "    X_tr.loc[segment, 'trend'] = add_trend_feature(x)\n",
    "    X_tr.loc[segment, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "    X_tr.loc[segment, 'abs_mean'] = np.abs(x).mean()\n",
    "    X_tr.loc[segment, 'abs_std'] = np.abs(x).std()\n",
    "    \n",
    "    X_tr.loc[segment, 'mad'] = x.mad()\n",
    "    X_tr.loc[segment, 'kurt'] = x.kurtosis()\n",
    "    X_tr.loc[segment, 'skew'] = x.skew()\n",
    "    X_tr.loc[segment, 'med'] = x.median()\n",
    "    \n",
    "    X_tr.loc[segment, 'Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "    X_tr.loc[segment, 'Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta5_mean'] = classic_sta_lta(x, 50, 1000).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta6_mean'] = classic_sta_lta(x, 100, 5000).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta7_mean'] = classic_sta_lta(x, 333, 666).mean()\n",
    "    X_tr.loc[segment, 'classic_sta_lta8_mean'] = classic_sta_lta(x, 4000, 10000).mean()\n",
    "    X_tr.loc[segment, 'Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n",
    "    ewma = pd.Series.ewm\n",
    "    X_tr.loc[segment, 'exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n",
    "    X_tr.loc[segment, 'exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n",
    "    X_tr.loc[segment, 'exp_Moving_average_30000_mean'] = ewma(x, span=30000).mean().mean(skipna=True)\n",
    "    no_of_std = 3\n",
    "    X_tr.loc[segment, 'MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n",
    "    X_tr.loc[segment,'MA_700MA_BB_high_mean'] = (X_tr.loc[segment, 'Moving_average_700_mean'] + no_of_std * X_tr.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "    X_tr.loc[segment,'MA_700MA_BB_low_mean'] = (X_tr.loc[segment, 'Moving_average_700_mean'] - no_of_std * X_tr.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "    X_tr.loc[segment, 'MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n",
    "    X_tr.loc[segment,'MA_400MA_BB_high_mean'] = (X_tr.loc[segment, 'Moving_average_700_mean'] + no_of_std * X_tr.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "    X_tr.loc[segment,'MA_400MA_BB_low_mean'] = (X_tr.loc[segment, 'Moving_average_700_mean'] - no_of_std * X_tr.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "    X_tr.loc[segment, 'MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n",
    "    X_tr.drop('Moving_average_700_mean', axis=1, inplace=True)\n",
    "    \n",
    "    X_tr.loc[segment, 'iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "    X_tr.loc[segment, 'q999'] = np.quantile(x,0.999)\n",
    "    X_tr.loc[segment, 'q001'] = np.quantile(x,0.001)\n",
    "    X_tr.loc[segment, 'ave10'] = stats.trim_mean(x, 0.1)\n",
    "\n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = x.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = x.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X_tr.loc[segment, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X_tr.loc[segment, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X_tr.loc[segment, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X_tr.loc[segment, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X_tr.loc[segment, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X_tr.loc[segment, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X_tr.loc[segment, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X_tr.loc[segment, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X_tr.loc[segment, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X_tr.loc[segment, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X_tr.loc[segment, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X_tr.loc[segment, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X_tr.loc[segment, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X_tr.loc[segment, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X_tr.loc[segment, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X_tr.loc[segment, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X_tr.loc[segment, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X_tr.loc[segment, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X_tr.loc[segment, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X_tr.loc[segment, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X_tr.loc[segment, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X_tr.loc[segment, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_tr2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr2 = X_tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtiny = 1e-6\n",
    "idx = X_tr < dtiny\n",
    "X_tr[idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = train.acoustic_data.mean()\n",
    "# std = train.acoustic_data.std()\n",
    "# low = mean - 3 * std\n",
    "# high = mean + 3 * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[(train.acoustic_data >= low) & (train.acoustic_data < high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 10000\n",
    "segments = int(np.floor(train.shape[0] / rows))\n",
    "\n",
    "X_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['ave', 'std', 'max', 'min'])\n",
    "y_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['time_to_failure'])\n",
    "\n",
    "for segment in tqdm(range(segments)):\n",
    "    seg = train.iloc[segment*rows:segment*rows+rows]\n",
    "    x = seg['acoustic_data'].values\n",
    "    y = seg['time_to_failure'].values[-1]\n",
    "    \n",
    "    y_train.loc[segment, 'time_to_failure'] = y\n",
    "    \n",
    "    X_train.loc[segment, 'ave'] = x.mean()\n",
    "    X_train.loc[segment, 'std'] = x.std()\n",
    "    X_train.loc[segment, 'max'] = x.max()\n",
    "    X_train.loc[segment, 'min'] = x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(n_input, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(num_features=n_hidden)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    # val_data, data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            # print(output.shape)\n",
    "            # print(targets.view(batch_size*seq_length, -1).shape)\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length, -1).float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.float().cuda(), targets.float().cuda()\n",
    "                    val_h = tuple([each.data for each in val_h])    \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "            \n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length, -1).float())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    features = data[:, 0:-1]\n",
    "    n_input_f = features.shape[1]\n",
    "    labels = data[:, -1]\n",
    "    try:\n",
    "        n_input_l = labels.shape[1]\n",
    "    except IndexError:\n",
    "        n_input_l = 1\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = features.shape[0]//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    features = features[:n_batches * batch_size_total]\n",
    "    labels = labels[:n_batches * batch_size_total]\n",
    "\n",
    "    # Reshape into batch_size rows\n",
    "    features = features.reshape((batch_size, -1, n_input_f))\n",
    "    labels = labels.reshape((batch_size, -1, n_input_l))\n",
    "\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, features.shape[1], seq_length):\n",
    "        x = features[:, n : n+seq_length, :]\n",
    "        y = labels[:, n : n+seq_length, :]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = dict()\n",
    "for column in X_tr.columns:\n",
    "    col = X_tr[column].values\n",
    "    nans[column] = np.isnan(col).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in nans:\n",
    "    if nans[key]:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_tr = X_tr.drop(['classic_sta_lta7_mean', 'classic_sta_lta5_mean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_np = new_X_tr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf(X_tr_np).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 136)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=120, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=120)\n",
    "pca.fit(X_tr_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"100_comp.csv\", pca.components_, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp_X = X_tr_np @ pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtiny = np.finfo(0.0).tiny\n",
    "idx = n_comp_X < dtiny\n",
    "n_comp_X[idx] = dtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack([n_comp_X, y_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack([X_train_scaled, y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarthquakeRNN(\n",
      "  (lstm): LSTM(136, 1024, num_layers=2, batch_first=True, dropout=0.7)\n",
      "  (dropout): Dropout(p=0.7)\n",
      "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden= 1024\n",
    "n_layers=2\n",
    "n_input = 136\n",
    "n_output=1\n",
    "\n",
    "net = EarthquakeRNN(n_input, n_output, n_hidden, n_layers, drop_prob=0.7)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4194, 136)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack([X_tr_np, y_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125/100000... Step: 500... Loss: 40.8875... Val Loss: 48.0304\n",
      "Epoch: 250/100000... Step: 1000... Loss: 37.1062... Val Loss: 66.2653\n",
      "Epoch: 375/100000... Step: 1500... Loss: 37.2283... Val Loss: 66.1109\n",
      "Epoch: 500/100000... Step: 2000... Loss: 36.2229... Val Loss: 58.7330\n",
      "Epoch: 625/100000... Step: 2500... Loss: 32.2784... Val Loss: 40.3923\n",
      "Epoch: 750/100000... Step: 3000... Loss: 33.2568... Val Loss: 33.1612\n",
      "Epoch: 875/100000... Step: 3500... Loss: 32.5015... Val Loss: 30.0274\n",
      "Epoch: 1000/100000... Step: 4000... Loss: 29.7737... Val Loss: 55.5785\n",
      "Epoch: 1125/100000... Step: 4500... Loss: 29.3484... Val Loss: 42.9657\n",
      "Epoch: 1250/100000... Step: 5000... Loss: 28.2487... Val Loss: 37.9592\n",
      "Epoch: 1375/100000... Step: 5500... Loss: 24.7917... Val Loss: 41.4882\n",
      "Epoch: 1500/100000... Step: 6000... Loss: 23.7404... Val Loss: 67.6469\n",
      "Epoch: 1625/100000... Step: 6500... Loss: 22.5342... Val Loss: 42.7297\n",
      "Epoch: 1750/100000... Step: 7000... Loss: 18.9137... Val Loss: 41.1740\n",
      "Epoch: 1875/100000... Step: 7500... Loss: 16.4485... Val Loss: 32.7400\n",
      "Epoch: 2000/100000... Step: 8000... Loss: 12.9185... Val Loss: 35.6474\n",
      "Epoch: 2125/100000... Step: 8500... Loss: 13.6583... Val Loss: 28.4715\n",
      "Epoch: 2250/100000... Step: 9000... Loss: 11.3836... Val Loss: 25.0157\n",
      "Epoch: 2375/100000... Step: 9500... Loss: 9.4500... Val Loss: 25.6562\n",
      "Epoch: 2500/100000... Step: 10000... Loss: 7.8454... Val Loss: 27.4980\n",
      "Epoch: 2625/100000... Step: 10500... Loss: 6.3666... Val Loss: 31.3428\n",
      "Epoch: 2750/100000... Step: 11000... Loss: 5.4930... Val Loss: 31.2463\n",
      "Epoch: 2875/100000... Step: 11500... Loss: 4.1123... Val Loss: 31.7581\n",
      "Epoch: 3000/100000... Step: 12000... Loss: 6.4052... Val Loss: 29.8966\n",
      "Epoch: 3125/100000... Step: 12500... Loss: 4.5858... Val Loss: 31.6982\n",
      "Epoch: 3250/100000... Step: 13000... Loss: 4.8754... Val Loss: 35.2618\n",
      "Epoch: 3375/100000... Step: 13500... Loss: 3.7191... Val Loss: 36.7637\n",
      "Epoch: 3500/100000... Step: 14000... Loss: 3.9288... Val Loss: 36.3944\n",
      "Epoch: 3625/100000... Step: 14500... Loss: 3.6174... Val Loss: 37.9661\n",
      "Epoch: 3750/100000... Step: 15000... Loss: 3.4434... Val Loss: 37.9635\n",
      "Epoch: 3875/100000... Step: 15500... Loss: 3.3830... Val Loss: 35.3011\n",
      "Epoch: 4000/100000... Step: 16000... Loss: 4.2162... Val Loss: 36.0008\n",
      "Epoch: 4125/100000... Step: 16500... Loss: 3.9631... Val Loss: 32.2141\n",
      "Epoch: 4250/100000... Step: 17000... Loss: 3.8477... Val Loss: 32.2316\n",
      "Epoch: 4375/100000... Step: 17500... Loss: 2.8920... Val Loss: 33.5146\n",
      "Epoch: 4500/100000... Step: 18000... Loss: 4.4466... Val Loss: 32.2968\n",
      "Epoch: 4625/100000... Step: 18500... Loss: 3.4869... Val Loss: 34.6445\n",
      "Epoch: 4750/100000... Step: 19000... Loss: 3.0724... Val Loss: 33.8903\n",
      "Epoch: 4875/100000... Step: 19500... Loss: 2.8882... Val Loss: 33.9751\n",
      "Epoch: 5000/100000... Step: 20000... Loss: 3.2179... Val Loss: 31.8913\n",
      "Epoch: 5125/100000... Step: 20500... Loss: 3.3008... Val Loss: 33.1654\n",
      "Epoch: 5250/100000... Step: 21000... Loss: 2.4117... Val Loss: 30.8730\n",
      "Epoch: 5375/100000... Step: 21500... Loss: 6.0353... Val Loss: 20.5346\n",
      "Epoch: 5500/100000... Step: 22000... Loss: 4.5485... Val Loss: 32.0388\n",
      "Epoch: 5625/100000... Step: 22500... Loss: 2.2711... Val Loss: 27.2569\n",
      "Epoch: 5750/100000... Step: 23000... Loss: 2.4012... Val Loss: 30.9867\n",
      "Epoch: 5875/100000... Step: 23500... Loss: 3.1985... Val Loss: 26.8204\n",
      "Epoch: 6000/100000... Step: 24000... Loss: 1.8793... Val Loss: 29.0684\n",
      "Epoch: 6125/100000... Step: 24500... Loss: 2.1001... Val Loss: 25.0768\n",
      "Epoch: 6250/100000... Step: 25000... Loss: 1.5367... Val Loss: 24.4868\n",
      "Epoch: 6375/100000... Step: 25500... Loss: 2.3901... Val Loss: 25.3626\n",
      "Epoch: 6500/100000... Step: 26000... Loss: 2.3845... Val Loss: 21.3241\n",
      "Epoch: 6625/100000... Step: 26500... Loss: 5.2152... Val Loss: 32.2332\n",
      "Epoch: 6750/100000... Step: 27000... Loss: 2.4218... Val Loss: 25.9589\n",
      "Epoch: 6875/100000... Step: 27500... Loss: 1.6371... Val Loss: 24.5976\n",
      "Epoch: 7000/100000... Step: 28000... Loss: 2.0621... Val Loss: 20.6096\n",
      "Epoch: 7125/100000... Step: 28500... Loss: 1.5976... Val Loss: 29.7138\n",
      "Epoch: 7250/100000... Step: 29000... Loss: 1.7614... Val Loss: 24.8158\n",
      "Epoch: 7375/100000... Step: 29500... Loss: 3.0194... Val Loss: 31.8235\n",
      "Epoch: 7500/100000... Step: 30000... Loss: 3.2175... Val Loss: 27.7195\n",
      "Epoch: 7625/100000... Step: 30500... Loss: 1.6593... Val Loss: 25.9606\n",
      "Epoch: 7750/100000... Step: 31000... Loss: 2.2019... Val Loss: 26.4863\n",
      "Epoch: 7875/100000... Step: 31500... Loss: 2.8910... Val Loss: 26.1369\n",
      "Epoch: 8000/100000... Step: 32000... Loss: 1.5376... Val Loss: 25.7664\n",
      "Epoch: 8125/100000... Step: 32500... Loss: 2.1060... Val Loss: 25.8788\n",
      "Epoch: 8250/100000... Step: 33000... Loss: 1.4053... Val Loss: 24.5758\n",
      "Epoch: 8375/100000... Step: 33500... Loss: 2.1752... Val Loss: 22.1954\n",
      "Epoch: 8500/100000... Step: 34000... Loss: 2.2555... Val Loss: 23.2383\n",
      "Epoch: 8625/100000... Step: 34500... Loss: 1.4195... Val Loss: 24.3766\n",
      "Epoch: 8750/100000... Step: 35000... Loss: 1.2451... Val Loss: 26.4034\n",
      "Epoch: 8875/100000... Step: 35500... Loss: 2.2578... Val Loss: 25.5313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2fff7034915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-0d7c7e1c5aa1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "seq_length = 40\n",
    "n_epochs = 100000 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, data, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.00001, print_every=500, val_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh = net.init_hidden(1)\n",
    "type(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(get_batches(data, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, hh = net(torch.from_numpy(x).float().cuda(), hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  x.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "resize only works on single-segment arrays",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c17abc3ac75f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: resize only works on single-segment arrays"
     ]
    }
   ],
   "source": [
    "y.resize(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae332f8f60>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(range(10000), y)\n",
    "plt.plot(range(100), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(40), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
