\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Property of derivatives of error function}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Sigmoid activation function}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Softmax activation function}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Updating weights}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Vanishing gradient problem}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Description}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Solutions}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Network}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Description}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Train loss and validation loss s decreasing\relax }}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Training}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Testing}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Discussion}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of predictions and real labels on test data\relax }}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces RMSProp\relax }}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Other optimization algorithms}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adam needs only 2000 iterations to achieve a decent accuracy (training loss is less than 0.06)\relax }}{5}}
